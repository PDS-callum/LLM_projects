{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "69d1b335",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "documents_to_load = [\n",
        "    \"../../test.pdf\"\n",
        "]\n",
        "\n",
        "def load_documents(documents_to_load: List[str]) -> List[Document]:\n",
        "    documents = []\n",
        "    for doc in documents_to_load:\n",
        "        if doc.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(doc)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file type: {doc}\")\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "documents = load_documents(documents_to_load)\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4eba630f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# Create a vector store on QDrant (uses split chunks for proper RAG retrieval)\n",
        "# Uses collection \"rag_documents\" - nomic-embed-text produces 768-dim embeddings\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "vector_store = QdrantVectorStore.from_texts(\n",
        "    texts=[doc.page_content for doc in splits],\n",
        "    embedding=embeddings,\n",
        "    metadatas=[doc.metadata for doc in splits],\n",
        "    url=\"http://localhost:6333\",\n",
        "    collection_name=\"rag_documents\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d2c34ea0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatBot(chat_history=[BaseMessage(content='The main idea of the document appears to be an award ceremony, specifically a scholarship awarded by A*STAR and the University of Southampton for doctoral studies in the UK and Singapore.', additional_kwargs={}, response_metadata={}, type='document_summary', score=0.745)], question='What is the main idea of the document?', answer='Awards 2019 - 2023| P H D S SCHOLARSHIP | A*STAR AND THE UNIVERSITY OF SOUTHAMPTON', chat_ended='No')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_ollama.chat_models import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Annotated, List, Literal\n",
        "import operator\n",
        "\n",
        "# Create the retriever to retrieve context from the vector store\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Create the model\n",
        "llm = ChatOllama(model=\"llama3.2\")\n",
        "\n",
        "# Create structure\n",
        "class ChatBot(BaseModel):\n",
        "    chat_history: Annotated[List[BaseMessage], operator.add]\n",
        "    question: str = Field(description=\"The question to be answered.\")\n",
        "    answer: str = Field(description=\"The answer to the question based on the context provided.\")\n",
        "    chat_ended: Literal[\"Yes\", \"No\"] = Field(description=\"Whether the chat has ended.\")\n",
        "\n",
        "structured_llm = llm.with_structured_output(ChatBot)\n",
        "\n",
        "# Create the template (use {context}, {question}, and {chat_history} as placeholders)\n",
        "template = \"\"\"\n",
        "Answer the question based on the following context and recent conversation history.\n",
        "\n",
        "Context from documents:\n",
        "{context}\n",
        "\n",
        "Recent conversation history:\n",
        "{chat_history}\n",
        "\n",
        "Current question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Helper function to format chat history as a string\n",
        "def format_chat_history(messages: List[BaseMessage]) -> str:\n",
        "    \"\"\"Format chat history messages into a readable string.\"\"\"\n",
        "    if not messages:\n",
        "        return \"No previous conversation.\"\n",
        "    \n",
        "    formatted = []\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            formatted.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            formatted.append(f\"Assistant: {msg.content}\")\n",
        "        else:\n",
        "            formatted.append(f\"{msg.type}: {msg.content}\")\n",
        "    \n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "# Create the rag chain with conversation history\n",
        "def create_rag_chain_with_history(chat_history: ChatMessageHistory):\n",
        "    \"\"\"Create a RAG chain that includes conversation history.\"\"\"\n",
        "    def get_chat_history_string(inputs: dict) -> dict:\n",
        "        # Get recent messages (last 10 messages to avoid token limits)\n",
        "        recent_messages = chat_history.messages[-10:] if len(chat_history.messages) > 10 else chat_history.messages\n",
        "        return {\n",
        "            **inputs,\n",
        "            \"chat_history\": format_chat_history(recent_messages)\n",
        "        }\n",
        "    \n",
        "    rag_chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | get_chat_history_string\n",
        "        | prompt\n",
        "        | structured_llm\n",
        "    )\n",
        "    \n",
        "    return rag_chain\n",
        "\n",
        "# Initialize conversation history\n",
        "conversation_history = ChatMessageHistory()\n",
        "\n",
        "# Create the rag chain with history support\n",
        "rag_chain = create_rag_chain_with_history(conversation_history)\n",
        "\n",
        "# Test the chain\n",
        "result = rag_chain.invoke(\"What is the main idea of the document?\")\n",
        "print(result)\n",
        "\n",
        "# Add the interaction to conversation history\n",
        "conversation_history.add_user_message(\"What is the main idea of the document?\")\n",
        "conversation_history.add_ai_message(result.answer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
